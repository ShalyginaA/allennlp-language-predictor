{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, LabelField, SequenceLabelField\n",
    "\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, TokenCharactersIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import allennlp\n",
    "allennlp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataReader(DatasetReader):\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                char_indexers: Dict[str, TokenCharactersIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.char_indexers = char_indexers or {\"token_characters\": TokenCharactersIndexer()}\n",
    "        self.all_letters = string.ascii_letters + \" .,;'\"\n",
    "        #the category_lines dictionary, a list of names per language\n",
    "        self.category_lines = {}\n",
    "        self.all_categories = []\n",
    "        self.n_categories = None\n",
    "        \n",
    "    # Turn a Unicode string to plain ASCII\n",
    "    def unicodeToAscii(self, s:str) -> str:\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            and c in self.all_letters)\n",
    "    \n",
    "    # Read a file and split into lines\n",
    "    def readLines(self, filename:str) -> str:\n",
    "        lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "        return [self.unicodeToAscii(line) for line in lines]\n",
    "    \n",
    "    #convert inputs corresponding to training example to Instance\n",
    "    def toInstance(self, names: List[str], categories: List[str] = None) -> Instance:\n",
    "        token_field = TextField([Token(nm) for nm in names], self.token_indexers)\n",
    "        \n",
    "        fields = {\"tokens\": token_field}\n",
    "        \n",
    "        fields[\"token_characters\"] = TextField([Token(nm) for nm in names], self.char_indexers)\n",
    "        \n",
    "        if categories:\n",
    "            fields[\"labels\"] = SequenceLabelField(labels=categories, sequence_field = token_field)\n",
    "        return Instance(fields)\n",
    "\n",
    "    #takes a filename and produces a stream of Instances (random training examples)\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        filenames = glob.glob(file_path)\n",
    "\n",
    "        for f in filenames:\n",
    "            category = os.path.splitext(os.path.basename(f))[0]\n",
    "            self.all_categories.append(category)\n",
    "            lines = self.readLines(f)\n",
    "            self.category_lines[category] = lines\n",
    "        self.n_categories = len(self.all_categories)\n",
    "        \n",
    "        name_and_category = []\n",
    "        for cat in self.all_categories:\n",
    "            for name in self.category_lines[cat]:\n",
    "                name_and_category.append((name,cat))\n",
    "                \n",
    "        np.random.shuffle(name_and_category)\n",
    "        \n",
    "        step = 10\n",
    "        \n",
    "        for i in range(0,len(name_and_category),step):\n",
    "            yield self.toInstance([n[0] for n in name_and_category[i:i+step]], \n",
    "                                  [n[1] for n in name_and_category[i:i+step]])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anastasiia/anaconda3/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:51: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "data_reader = NameDataReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2008it [00:00, 6224.36it/s]\n"
     ]
    }
   ],
   "source": [
    "data = data_reader.read('data/names/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Korean',\n",
       " 'Arabic',\n",
       " 'Russian',\n",
       " 'Russian',\n",
       " 'Russian',\n",
       " 'English',\n",
       " 'Russian',\n",
       " 'Russian',\n",
       " 'Russian',\n",
       " 'Russian']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].fields['labels'].labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n",
    "from allennlp.modules.token_embedders.token_characters_encoder import TokenCharactersEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2008/2008 [00:00<00:00, 7287.81it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  tokens, Size: 17424 || token_characters, Size: 57 || labels, Size: 18 || Non Padded Namespaces: {'*tags', '*labels'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameNet(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self,\n",
    "               tokens: Dict[str, torch.Tensor],\n",
    "               token_characters: Dict[str, torch.Tensor],\n",
    "               labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        \n",
    "        mask = get_text_field_mask(tokens)\n",
    "        \n",
    "        embeddings = self.word_embeddings({**tokens,**token_characters})\n",
    "        \n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        \n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        \n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        \n",
    "        if labels is not None:\n",
    "            self.accuracy(tag_logits, labels, mask)\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data,test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_EMB_DIM = 5\n",
    "WORD_EMB_DIM = 5\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_encoder = PytorchSeq2VecWrapper(torch.nn.RNN(CHAR_EMB_DIM, CHAR_EMB_DIM, batch_first=True))\n",
    "token_char_embedding = Embedding(num_embeddings=vocab.get_vocab_size('token_characters'),\n",
    "                            embedding_dim=WORD_EMB_DIM)\n",
    "char_embeddings = TokenCharactersEncoder(token_char_embedding, char_encoder)\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=WORD_EMB_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding, \"token_characters\": char_embeddings})\n",
    "\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = NameNet(word_embeddings, lstm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4597, loss: 1.9598 ||: 100%|██████████| 803/803 [00:05<00:00, 136.44it/s]\n",
      "accuracy: 0.4689, loss: 1.8376 ||: 100%|██████████| 201/201 [00:00<00:00, 276.71it/s]\n",
      "accuracy: 0.4686, loss: 1.8519 ||: 100%|██████████| 803/803 [00:06<00:00, 125.31it/s]\n",
      "accuracy: 0.4689, loss: 1.8305 ||: 100%|██████████| 201/201 [00:00<00:00, 488.49it/s]\n",
      "accuracy: 0.4686, loss: 1.8391 ||: 100%|██████████| 803/803 [00:06<00:00, 126.74it/s]\n",
      "accuracy: 0.4689, loss: 1.7969 ||: 100%|██████████| 201/201 [00:00<00:00, 511.00it/s]\n",
      "accuracy: 0.4712, loss: 1.7069 ||: 100%|██████████| 803/803 [00:05<00:00, 142.24it/s]\n",
      "accuracy: 0.4856, loss: 1.5815 ||: 100%|██████████| 201/201 [00:00<00:00, 429.29it/s]\n",
      "accuracy: 0.5204, loss: 1.5385 ||: 100%|██████████| 803/803 [00:05<00:00, 139.13it/s]\n",
      "accuracy: 0.5414, loss: 1.4712 ||: 100%|██████████| 201/201 [00:00<00:00, 445.99it/s]\n",
      "accuracy: 0.5398, loss: 1.4778 ||: 100%|██████████| 803/803 [00:06<00:00, 115.26it/s]\n",
      "accuracy: 0.5476, loss: 1.4389 ||: 100%|██████████| 201/201 [00:00<00:00, 478.88it/s]\n",
      "accuracy: 0.5449, loss: 1.4412 ||: 100%|██████████| 803/803 [00:07<00:00, 111.87it/s]\n",
      "accuracy: 0.5698, loss: 1.3940 ||: 100%|██████████| 201/201 [00:00<00:00, 439.79it/s]\n",
      "accuracy: 0.5792, loss: 1.3915 ||: 100%|██████████| 803/803 [00:06<00:00, 91.95it/s] \n",
      "accuracy: 0.5959, loss: 1.3418 ||: 100%|██████████| 201/201 [00:00<00:00, 315.28it/s]\n",
      "accuracy: 0.5907, loss: 1.3462 ||: 100%|██████████| 803/803 [00:07<00:00, 107.04it/s]\n",
      "accuracy: 0.6116, loss: 1.3038 ||: 100%|██████████| 201/201 [00:00<00:00, 420.04it/s]\n",
      "accuracy: 0.6006, loss: 1.3174 ||: 100%|██████████| 803/803 [00:06<00:00, 125.15it/s]\n",
      "accuracy: 0.6114, loss: 1.2892 ||: 100%|██████████| 201/201 [00:00<00:00, 452.22it/s]\n",
      "accuracy: 0.6128, loss: 1.2976 ||: 100%|██████████| 803/803 [00:06<00:00, 116.81it/s]\n",
      "accuracy: 0.6363, loss: 1.2556 ||: 100%|██████████| 201/201 [00:00<00:00, 428.23it/s]\n",
      "accuracy: 0.6193, loss: 1.2760 ||: 100%|██████████| 803/803 [00:07<00:00, 114.15it/s]\n",
      "accuracy: 0.6380, loss: 1.2356 ||: 100%|██████████| 201/201 [00:00<00:00, 442.81it/s]\n",
      "accuracy: 0.6318, loss: 1.2571 ||: 100%|██████████| 803/803 [00:07<00:00, 110.34it/s]\n",
      "accuracy: 0.6492, loss: 1.2150 ||: 100%|██████████| 201/201 [00:00<00:00, 390.91it/s]\n",
      "accuracy: 0.6357, loss: 1.2392 ||: 100%|██████████| 803/803 [00:05<00:00, 138.72it/s]\n",
      "accuracy: 0.6550, loss: 1.1939 ||: 100%|██████████| 201/201 [00:00<00:00, 466.98it/s]\n",
      "accuracy: 0.6412, loss: 1.2232 ||: 100%|██████████| 803/803 [00:07<00:00, 110.07it/s]\n",
      "accuracy: 0.6627, loss: 1.1769 ||: 100%|██████████| 201/201 [00:00<00:00, 361.47it/s]\n",
      "accuracy: 0.6455, loss: 1.2015 ||: 100%|██████████| 803/803 [00:07<00:00, 103.34it/s]\n",
      "accuracy: 0.6642, loss: 1.1594 ||: 100%|██████████| 201/201 [00:00<00:00, 358.56it/s]\n",
      "accuracy: 0.6550, loss: 1.1734 ||: 100%|██████████| 803/803 [00:06<00:00, 120.34it/s]\n",
      "accuracy: 0.6667, loss: 1.1330 ||: 100%|██████████| 201/201 [00:00<00:00, 411.97it/s]\n",
      "accuracy: 0.6733, loss: 1.1249 ||: 100%|██████████| 803/803 [00:06<00:00, 131.13it/s]\n",
      "accuracy: 0.6953, loss: 1.0701 ||: 100%|██████████| 201/201 [00:00<00:00, 383.55it/s]\n",
      "accuracy: 0.6977, loss: 1.0630 ||: 100%|██████████| 803/803 [00:06<00:00, 129.69it/s]\n",
      "accuracy: 0.7080, loss: 1.0235 ||: 100%|██████████| 201/201 [00:00<00:00, 458.48it/s]\n",
      "accuracy: 0.7060, loss: 1.0199 ||: 100%|██████████| 803/803 [00:06<00:00, 119.74it/s]\n",
      "accuracy: 0.7040, loss: 1.0148 ||: 100%|██████████| 201/201 [00:00<00:00, 376.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 19,\n",
       " 'peak_cpu_memory_MB': 297.028,\n",
       " 'training_duration': '00:02:23',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 19,\n",
       " 'epoch': 19,\n",
       " 'training_accuracy': 0.7060398505603985,\n",
       " 'training_loss': 1.0199193247636853,\n",
       " 'training_cpu_memory_MB': 297.028,\n",
       " 'validation_accuracy': 0.7040358744394619,\n",
       " 'validation_loss': 1.0147619629973796,\n",
       " 'best_validation_accuracy': 0.7040358744394619,\n",
       " 'best_validation_loss': 1.0147619629973796}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"tokens\", \"num_tokens\"), (\"token_characters\", \"num_token_characters\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train,\n",
    "                  validation_dataset=val,\n",
    "                  patience=5,\n",
    "                  num_epochs=20, cuda_device=-1)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "from allennlp.common.util import JsonDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguagePredictor(Predictor):\n",
    "    def predict_json(self,inputs: JsonDict) -> JsonDict:\n",
    "        instance = self._dataset_reader.toInstance(inputs)\n",
    "        out = self.predict_instance(instance)\n",
    "        #find maximum score from predictions\n",
    "        return [self._model.vocab.get_token_from_index(i,'labels') \n",
    "                for i in np.argmax(out['tag_logits'], axis=-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict name category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = LanguagePredictor(model,data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Japanese', 'Russian', 'English', 'Arabic']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict_json(['Takahashi','Sokolov','Foster','Abboud'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
